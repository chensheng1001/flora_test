# Fine-tuning GPT-2 on WikiText-2 dataset

## Prerequisites
Install `pytorch` with GPU support following instructions on [official website](https://pytorch.org/).

Install `transformers` and `datasets`
```
pip install transformers==4.35.1
pip install datasets==2.15.0
```

## Runing
To fine-tune a pretrained model, run:
```
python codes/train.py
```
This will use pretrained model and tokenizer in `ckpts` folder and will download dataset from Hugging Face. 
The pretrained model and tokenizer are also downloaded from Hugging Face.

To inference your fine-tuned model, run:
```
python codes/inference.py
```
Then you should input your prompt and the maximum length of output. You can get the text generated by the model, for example:
```
oil price
20
oil price decline was blamed on the loss of the rubber industry in Asia, which is responsible for an
```
To evaluate your model by calculating perplexity on test dataset, run:
```
python codes/evaluate.py
```

## References

- [Fine-tuning GPT-2 for Beginners](https://www.kaggle.com/code/changyeop/how-to-fine-tune-gpt-2-for-beginners): A Kaggle notebook providing step-by-step guidance on GPT-2 fine-tuning for beginners.
- [Computing Perplexity on Hugging Face](https://github.com/huggingface/transformers/issues/9648): A GitHub discussion on computing perplexity using the Hugging Face library.
- [Zhihu Article on GPT-2 Fine-tuning](https://zhuanlan.zhihu.com/p/647862375?utm_id=0): An insightful article on GPT-2 fine-tuning from Zhihu.